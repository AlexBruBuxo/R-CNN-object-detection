"""
This scrip takes the raccoon images and creates a separate raccoon/no_raccoon dataset, which we will use to 
fine-tune a MobileNet V2 model that is pre-trained on the ImageNet dataset.

1. Accept our input raccoons dataset
2. Loop over all images in the dataset
2a. Load the given input image
2b. Load and parse the bounding box coordinates for any raccoons in the input image
3. Run Selective Search on the input image
4. Use IoU to determine which region proposals from Selective Search sufficiently overlap with the 
ground-truth bounding boxes and which ones do not
5. Save region proposals as overlapping (contains raccoon) or not (no raccoon)

We have to generate a new dataset through selective serch for the CNN to learn the images as they will be 
processed on inference, therefore the dataset must be generated from selective search. This means that some 
of the resultant images are similar to each other and in some cases are near-duplicates — that is in fact 
the intended behavior. It’s totally feasible that Selective Search could fire multiple times in the similar 
regions. We could add additional logic that can be used to filter out regions that significantly overlap 
(TODO).

Source: https://pyimagesearch.com/2020/07/13/r-cnn-object-detection-with-keras-tensorflow-and-deep-learning/   

This file can be executed as follows:
$ (time) python v4_build_dataset.py
"""

from helpers.iou import compute_iou
from helpers import config
from bs4 import BeautifulSoup
from imutils import paths
import cv2
import os

# loop over the output positive and negative directories
for dirPath in (config.POSITIVE_PATH, config.NEGATIVE_PATH):
    # if the output directory does not exist yet, create it
    if not os.path.exists(dirPath):
        os.makedirs(dirPath)
    
# grab all image paths in the input image directory
# "paths.list_images" is a generator
imagePaths = list(paths.list_images(config.ORIG_IMAGES))

# initialize the total number of positive and negative images we have
totalPositive = 0
totalNegative = 0

# loop over the image paths
for (i, imagePath) in enumerate(imagePaths):
    # show a progress report
    print("[INFO] processing image {}/{}".format(i +1, 
        len(imagePaths)))
    
    # extract the filename from the file path and use it to derive
    # the path to the XML annotation file
    filename = imagePath.split(os.path.sep)[-1]  # extract name
    filename = filename[:filename.rfind(".")]  # remove extension
    annotPath = os.path.sep.join([config.ORIG_ANNOTS,
        "{}.xml".format(filename)])
    
    # load the annotation file, build the soup, and initialize our
    # list of ground-truth bounding boxes
    # annotation file has PASCAL VOC XML format
    contents = open(annotPath).read()
    soup = BeautifulSoup(contents, "html.parser")
    gtBoxes = []  # ground-truth boxes
    
    # extract the image dimensions
    # "soup.find("width")" => "<width>650</width>"
    # "soup.find("width").string" => "'650'"
    w = int(soup.find("width").string)
    h = int(soup.find("height").string)
    
    # loop over all 'object' elements and extract ground-truth boxes
    for o in soup.find_all("object"):
        # extract the label and bounding box coordinates
        label = o.find("name").string
        xMin = int(o.find("xmin").string)
        yMin = int(o.find("ymin").string)
        xMax = int(o.find("xmax").string)
        yMax = int(o.find("ymax").string)
        
        # truncate any bounding box coordinates that may fall
        # outside the boundaries of the image
        xMin = max(0, xMin)
        yMin = max(0, yMin)
        xMax = min(w, xMax)
        yMax = min(h, yMax)
        
        # update our list of ground-truth bounding boxes
        gtBoxes.append((xMin, yMin, xMax, yMax))
        
    # load the input image from disk
    image = cv2.imread(imagePath)
    
    # we the result of selective search and validate the output with
    # the ground-truth boxes in order to train the CNN with the same
    # type of ROIs; if we would train it directly with the ground-
    # truth, the trained CNN would not generalize to other ROIs
    
    # run selective search on the image and initialize our list of
    # proposed boxes
    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()
    ss.setBaseImage(image)
    ss.switchToSelectiveSearchFast()
    rects = ss.process() # generated rectangles
    proposedRects = []
    
    # loop over the rectanfles generated by selective search
    for (x, y, w_, h_) in rects:
        # convert our bounding boxes from (x, y, w, h) to (startX,
        # startY, endX, endY)
        proposedRects.append((x, y, x + w_, y + h_))
        
    # initialize counters used to count the number of positive and 
    # negative ROIs saved thus far
    # positiveROIs => number of region proposals that sufficiently 
    # overlap with ground-truth annotations (saved in 
    # config.POSITIVE_PATH)
    # negativeROIs => number of region proposals that fail to meet 
    # our IoU threshold of 70% (saved to config.NEGATIVE_PATH)
    positiveROIs = 0  # raccoon
    negativeROIs = 0  # no_raccoon
    
    # loop over the maximum number of region proposals
    for proposedRect in proposedRects[:config.MAX_PROPOSALS]:
        # unpack the proposed rectangle bounding boxes
        (propStartX, propStartY, propEndX, propEndY) = proposedRect
        
        # loop over the ground-truth bounding boxes
        for gtBox in gtBoxes:
            # compute the Intersection over Union between the two
            # boxes and unpack the ground-truth bounding box
            iou = compute_iou(gtBox, proposedRect)
            (gtStartX, gtStartY, gtEndX, gtEndY) = gtBox
            
            # initialize the ROI and output path
            roi = None
            outputPath = None
            
            # check to see if the IoU is greater than 70% *and* that
            # we have not hit our positive count limit
            if iou > 0.7 and positiveROIs <= config.MAX_POSITIVE:
                # extract the ROI and then derive the output path to 
                # the positive instance
                roi = image[propStartY:propEndY, propStartX:propEndX]
                filename = "{}.png".format(totalPositive)
                outputPath = os.path.sep.join([config.POSITIVE_PATH, 
                    filename])
                
                # increment the positive counters
                positiveROIs += 1
                totalPositive += 1
            
            # to determine if the proposedRect and gtBox pair is 
            # negative ROI, we need to check if there's overlap;
            # determine if the proposed bounding box falls entirely
            # inside the ground-truth bounding box (i.e., fullOverlap)
            fullOverlap = propStartX >= gtStartX
            fullOverlap = fullOverlap and propStartY >= gtStartY
            fullOverlap = fullOverlap and propEndX <= gtEndX
            fullOverlap = fullOverlap and propEndY <= gtEndY
            
            # check to see if there is not full overlap *and* the IoU
            # is less than 5% *and* we have not hit our negative
            # count limit
            if not fullOverlap and iou < 0.05 and \
                negativeROIs <= config.MAX_NEGATIVE:
                # extract the ROI and then derive the output path to
                # the negative instance
                roi = image[propStartY:propEndY, propStartX:propEndX]
                filename = "{}.png".format(totalNegative)
                outputPath = os.path.sep.join([config.NEGATIVE_PATH,
                    filename])
                
                # increment the negative counters
                negativeROIs += 1
                totalNegative += 1
            
            # check to see if both ROI and output path are valid
            if roi is not None and outputPath is not None:
                # resize the ROI to the input dimensions of the CNN
                # that we'll be fine-tuning, then write the ROI to
                # disk
                roi = cv2.resize(roi, config.INPUT_DIMS,
                    interpolation=cv2.INTER_CUBIC)
                cv2.imwrite(outputPath, roi)